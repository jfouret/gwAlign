#!/usr/bin/env python
import argparse
version='SEDMATCHGITVERSION'
year=2018
author='Julien Fouret'
contact='julien@fouret.me'

##parse argument

parser = argparse.ArgumentParser(description=
"""
Genome-Wide Alignments - Unify

From Exons to well-aligned and filtered CDS
                        ---------------
Steps automatized on HPC infrastructure with upype python package
For more informations:
https://fouret.me/gitea/jfouret/upype
                        ---------------
-------------------------------------------------------------------
mode:create         First create a file architecture to extract CDS 
                    from a database of MSA (Multiple Sequence 
                    Alignment) at exon-level formatted like UCSC.
-------------------------------------------------------------------
mode:getCDS         Then reassemble all exons into one CDS avoiding 
                    reproducing mistakes in the parent file. 
-------------------------------------------------------------------
mode:annotate       Optionally get the corresponding name (HUGO) of
                    the gene with the accession ids (UCSC, NCBI and 
                    Uniprot) and add kegg, go and uniprot 
                    annotations based on a UCSC-like  mySQL 
                    database. 
-------------------------------------------------------------------
mode:align          re-align all CDS with a coding-aware method 
                    (macse). 
-------------------------------------------------------------------
mode:compute_TCS    Compute TCS (Transitive consistency score) with
                    t_coffee for each alignment
-------------------------------------------------------------------
mode:blast_search   Search in Trembl and sp the sequence from the
                    reference species to associate an id.            
-------------------------------------------------------------------
mode:stat_filter    Use TCS score to filter MSA and to produce 
                    statistics on sequence qualities by species.
-------------------------------------------------------------------
"""
,epilog="Version : "+str(version)+"\n"+str(year)+"\nAuthor : "+author+" for more informations or enquiries please contact "+contact,formatter_class=argparse.RawTextHelpFormatter)

parser.add_argument('-mode', metavar='keyword', required=True, help="Only one by one")


parser.add_argument('-out', metavar='/path', required=True, help="[all] folder in which to write all computed alignments ")
parser.add_argument('-v',action='store_true', help="[all] verbose")
parser.add_argument('-debug',action='store_true', help="[all] more verbose")


parser.add_argument('-al', metavar='/path', required=False, help="[create/getCDS] alignment file in fasta format ")
parser.add_argument('-spec', metavar='/path' , required=False, help="""[getCDS] tab file with assemblies info (name:path) 
(with .dict and .fai)""")
parser.add_argument('-ref', metavar='name', required=False, help="""[create/align/compute_TCS/stat_filter] reference species for multiple alignment 
mode annotate hase been design and tested only for hg19
(default='hg19')""",default='hg19')


parser.add_argument('-queue', metavar='queue', required=False, help="""[getCDS/align/compute_TCS] queue for PBS 
(default='SEDMATCHQUEUE')""",default='SEDMATCHQUEUE')
parser.add_argument('-batchLim', metavar='N', default='10', help="""[getCDS/align/compute_TCS] Number of operation per batch
(default='10')""")
parser.add_argument('-mems', metavar='lm,rm,hm', default='10,3,256', help="""[align] Precise memory config in GB for alignments:
>lm is the basic parameter to use first for alignments
>rs is what jobs will reserve 
>hm is an increased amount of memory for failures
default='10,3,256'""")

parser.add_argument('-keep',action='store_true', help="[align/compute_TCS] Only files with missing target (subsequent errors)")

parser.add_argument('-name', metavar='name' , required=False, help="""[stat_filter] sub-folder with fitered data 
(default='default_filter')""",default='default_filter')
parser.add_argument('-host', metavar='IP' , required=False, help="""[annotate] mysql host for ucsc database 
(default='10.0.0.200')""",default='10.0.0.200')
parser.add_argument('-port', metavar='PORT' , required=False, help="""[annotate]mysql port for ucsc database
(default='3306')""",default='3306')

parser.add_argument('-blast_db', metavar='path', required=False, help="""[blast_search] Blast database
with Trembl and Sprot sequences from ref species
(default=None)""",default=None)

parser.add_argument('-cpu', metavar='N', default='1', help="""[stat_filter] Number of cpu to be used
(default='1')""")
parser.add_argument('-phase_filter', metavar='N,N', required=False, help="""[stat_filter] Min TCS score with:
1: consensus sequence
2: each sequences 
(default='5,3')""",default='5,3')
parser.add_argument('-ratio', metavar='N', required=False, help="""[stat_filter] Min ratio species conserved in MSA
min ratio of species conseserve/all species considered
(default='0.6')""",default='0.6')
parser.add_argument('-focus', metavar='name,name', required=False, help="""[stat_filter] sub list of species that matters
the ratio threshold is re-applied on this sub-list.
comma-separated
(default=None)""",default=None)

parser.add_argument('-picard', metavar='/path', required=False, help="""[getCDS] picard jar path 
(default='SEDMATCHPICARD')""",default='SEDMATCHPICARD')
parser.add_argument('-macse', metavar='/path', required=False, help="""[align] macse jar path 
(default='SEDMATCHMACSE')""",default='SEDMATCHMACSE')


args=parser.parse_args()

import os
import sys
import mysql.connector
from Bio import SeqIO
from Bio.Seq import Seq
import re
from upype import *

##define function
def writedb(query,file_name,header):
	global cnx
	global args
	if args.v:
		print query
		print("\n")
	cursor=cnx.cursor()
	cursor.execute(query)
	rows=cursor.fetchall()
	tab_file=open(file_name,"w")
	tab_file.write("\t".join(header))
	for row in rows:
		tab_file.write("\n")
		tab_file.write("\t".join(map(str,row)))
	tab_file.close()
	cursor.close()
def Get_annotation(ID):
	"""
	@summary: Take a kgID as input to search the ucsc database for name, cross reference ids and functional information for KEGG and GO annotation spaces. All information are written in other files.
	@param ID: a kgID, kg stand for knowGene
	@type ID: str  
	"""
	global args
	global iterNoName
	global allSymbol
	field='kgID'
	#global ID_type
	query_where='WHERE (ref.'+field+'="'+ID+'");'
	query_name=['SELECT hgnc.symbol,ref.kgID,ref.spID,ref.refseq,ref.geneSymbol']
	query_name.append('FROM kgXref ref')
	query_name.append('LEFT JOIN proteinDB.hgncXref hgnc ON ( hgnc.uniProt=ref.spID ) AND (hgnc.refSeq=ref.refseq)')
	query_name.append(query_where)

	query_alias=['SELECT ref.alias']
	query_alias.append('FROM kgAlias ref')
	query_alias.append(query_where)

	query_uniprot=['SELECT ref.geneSymbol,feats.start,feats.end,class.val AS `class`,type.val AS `type`,feats.softEndBits AS `OutOfRange`']
	query_uniprot.append('FROM uniProt.feature feats')
	query_uniprot.append('LEFT JOIN kgXref ref ON (ref.spID=feats.acc)')
	query_uniprot.append('LEFT JOIN uniProt.featureClass class ON (class.id=feats.featureClass)')
	query_uniprot.append('LEFT JOIN uniProt.featureType type ON (type.id=feats.featureType)')
	query_uniprot.append(query_where)

	query_kegg=['SELECT ref.geneSymbol,kegg.mapID,des.description']
	query_kegg.append('FROM keggPathway kegg')
	query_kegg.append('LEFT JOIN kgXref ref ON (ref.kgID=kegg.kgID)')
	query_kegg.append('LEFT JOIN keggMapDesc des ON (kegg.mapID=des.mapID)')
	query_kegg.append(query_where)

	query_go=['SELECT ref.geneSymbol, goa.goId, goterm.name, goterm.term_type']
	query_go.append('FROM go.goaPart goa')
	query_go.append('LEFT JOIN kgXref ref ON (ref.spID=goa.dbObjectId)')
	query_go.append('LEFT JOIN go.term goterm ON (goa.goId=goterm.acc)')
	query_go.append(query_where)

	annotPath=rootedDir.results+"/"+ID+"/annotation/"
	mkdirp(annotPath)
	writedb(" ".join(query_name),annotPath+"/name.tab",['geneSymbol','kgID','uniprot','refSeq','kgName'])
	name_file=open(annotPath+"/name.tab","r")
	name_file_line1=name_file.readlines()[1]
	gene_symbol=name_file_line1.split("\t")[0]
	hgnc=name_file_line1.split("\t")[0]
	spID=name_file_line1.split("\t")[2]
	refseq=name_file_line1.split("\t")[3]
	kgname=name_file_line1.split("\t")[4]
	gene_symbol=hgnc
	if (hgnc=='None' or hgnc=='') or (spID=='' and refseq=='') :
		gene_symbol='kg_'+kgname
	elif kgname=='None' or kgname=='':
		gene_symbol='sp_'+spID
	elif spID=='None' or spID=='':
		gene_symbol='rs_'+refseq
	elif refseq=='None' or refseq=='':
		gene_symbol='NoID_'+str(iterNoName)
		iterNoName+=1
	gene_symbol=renameFileName(gene_symbol)
	if (gene_symbol in allSymbol.keys()):
		allSymbol[gene_symbol]+=1
		gene_symbol='dup'+str(allSymbol[gene_symbol])+'_'+gene_symbol
	else:
		allSymbol[gene_symbol]=0
	name_file.close()
	with  open(annotPath+"/consName.txt",'w') as consNameFIle:
		consNameFIle.write(gene_symbol)
	#os.system('mv '+rootedDir.results+"/"+ID+".name.tab"+' '+rootedDir.results+"/"+gene_symbol+'-'+ID+".name.tab")
	writedb(" ".join(query_uniprot),annotPath+"/uniprot.tab",['geneSymbol','start','end','class','type','OutOfRange'])
	writedb(" ".join(query_kegg),annotPath+"/kegg.tab",['geneSymbol','mapID','description'])
	writedb(" ".join(query_go),annotPath+"/go.tab",['geneSymbol','goId','name','type'])
	writedb(" ".join(query_alias),annotPath+"/alias.txt",['alias'])
	return gene_symbol
#start program

#parse args
modeList=args.mode.split(',')

### Create directories and listing...
kgPathDict=dict()
if 'create' in modeList:
	alnFileName=os.path.abspath(args.al)
	rootedDir=RootDir(args.out,batch=True)
	rootedDir.logs.writeArgs(args)
	# iter and create and print ref.fa
	rehg19Exon1=re.compile('^>(uc\w*)(\.\d*)_'+args.ref+'_1_(\d*) (\d*) (\d*) (\d*)\s?(\w+:\d+-\d+[+-]{1};?)*$')
	exons_file=open(alnFileName,'r')
	for line in exons_file.readlines():
		m=rehg19Exon1.match(line.rstrip())
		if m:
			if args.v:
				out_logger('create - Create_dir '+line)
			kgID=m.group(1)+m.group(2)
			path=rootedDir.results+"/"+kgID
			kgPathDict[kgID]=path
			mkdirp(path)
	with open(rootedDir.reports+'/kgPathDictFile.tab','w') as kgPathDictFile:
		for key in kgPathDict.keys():
			kgPathDictFile.write(key+"\t"+kgPathDict[key]+"\n")
	saveRoot(rootedDir)
else:
	if args.debug:
		rootedDir=RootDir(args.out,batch=True)
		rootedDir.logs.writeArgs(args)
	else:
		rootedDir=loadRoot(args.out)
	kgPathDict=dict()
	with open(rootedDir.reports+'/kgPathDictFile.tab','r') as kgPathDictFile:
		for line in kgPathDictFile.readlines():
			key,value=line.rstrip().split("\t")
			kgPathDict[key]=value

# get CDS
#########################
###### Explanation ######
# All this workflow is based on a file from ucsc which contains all sequences matching any hg19 knowngene \
# However this file has a major problem:  no gap have been considered in hg19 sequences. Meaning that     \
# only deletion are considered in other species and insertion are not reported in this files.
# Fortunately there is one fasta entry per exon per species and the entry name gives the genomic location \
# of the each exon is the corresponding species' genome. 
# The next lines of codes focuses on parsing this files for genomic coordinates aiming at using Picard    \ 
# to extract sequences from the genomes.
#
# In addition to this first issue, 5 cases are possible with those coordinates
# 	1- The coordinates are complete, fragmented or not, and consistent meaning that start<end thus        \
# 	those locations are appended to a bed file that will finally used to produce a cds.fa file.
# 	2- The coordinates are missing then a folder is created with '---' sequence in a cds.fa file
# 	3- The coordinates are not consistent. start>end ... well I suppose the outer sequences should be     \
# 	considered. Due to lack of time this feature has not been implemented yet and original sequences are  \
# 	added in a cds.fa file (with potential insertions missing...)
# 	4- The species is te reference (hg19) thus original sequences from input file have been used
# 	5- In place of the genomic location the mention "mapped" is present. This means that exons have been  \
# 	assembled using mapping then platypus thus orginals sequences have been used.

if 'getCDS' in modeList:
	alnFileName=os.path.abspath(args.al)
	specDict=dict()
	with open(args.spec,'r') as species_file:
		for line in species_file.readlines():
			if line.rstrip()!='':
				if args.v:
					out_logger('getCDS-Pasing -spec argument : line :'+line.rstrip())
				specie,assembly=line.rstrip().split("\t")
				specDict[specie]=assembly
	rootedDir.addCommand('java','java -XX:ParallelGCThreads=1','java -version 2>&1 | xargs')

	picard_cmd=rootedDir.getCommand("java",options={'-jar':args.picard})
	rootedDir.addCommand("picard",picard_cmd,picard_cmd+' CheckFingerprint --version 2>&1 | sed \'s/(.*$//g\'')
	reID=re.compile('^(uc\w*)(\.\d*)_(\w*)_(\d*)_(\d*) (\d*) (\d*) (\d*)\s*(\w+.*)?$')
	fasta_sequences = SeqIO.parse(open(alnFileName),'fasta')

	cmdList=list()
	iterBatch=1
	Nlim=int(args.batchLim)

	serialBatch=rootedDir.addSerializer("gwAlign_getCDS",Nlim,queue=args.queue,ncpus='1',mem="6gb",workdir=rootedDir.results,verbose=args.v)

	init=True

	# step 1 - iteration over exon file
	total_entry=len(SeqIO.to_dict(SeqIO.parse(open(alnFileName),'fasta')).keys())
	if args.v : out_logger("getCDS - "+str(total_entry)+" fasta entry to parse.")
	count=0

	def getCDS(cmdList):

		global args
		global kgPathDict
		global specDict
		global rootedDir

		if args.v:
			out_logger("getCDS - Getting CDS of "+oldkgID)
		oldkgPath=kgPathDict[oldkgID]
		for key in specDict.keys():
			oldRegFileName=oldkgPath+'/'+key+'/region.list'
			#TODO si regionlist exist
			if args.v:
				out_logger("getCDS - Getting CDS of "+oldkgID+" --- Focus on species: "+key)
			if os.path.isfile(oldRegFileName+'.tmp'):
				getRefOpt={
					'R':specDict[key],
					'O':oldkgPath+'/'+key+'/cds.fa',
					'INTERVAL_LIST':oldRegFileName,
					'VERBOSITY':'ERROR',
					'QUIET':'true'
				}
				cmdList.append('cat '+specDict[key].rstrip('fa').rstrip('fasta').rstrip('fna')+'dict '+oldRegFileName+'.tmp'+' > '+oldRegFileName+' ; rm '+oldRegFileName+'.tmp')
				cmdList.append(rootedDir.getCommand("picard",options=getRefOpt,subprogram='ExtractSequences',sep='='))
				cmdList.append('rm '+oldRegFileName)
		if args.v: out_logger("getCDS - Getting CDS of "+oldkgID+" --> serialize")
		rootedDir.serialize("gwAlign_getCDS",cmdList)
		

	for fasta in fasta_sequences:
		count+=1
		# step 1.2 - get name and sequence
		name, sequence = fasta.description, str(fasta.seq)
		m=reID.match(name)
		if m and (m.group(3) in specDict.keys()):
			spec=m.group(3)
			kgID=m.group(1)+m.group(2)
			if args.v : 
				out_logger("getCDS parsing input fasta file for id="+kgID+"\tspecies="+spec+"\texon="+m.group(4))
			if init:
				oldkgID=kgID
				init=False
			if kgID!=oldkgID:
				getCDS(cmdList)
				cmdList=list()
				oldkgID=kgID
			mkdirp(kgPathDict[kgID]+'/'+spec)
			if m.group(9)==None:
				mkdirp(kgPathDict[kgID]+'/'+spec+'/missingExon'+m.group(4))
				cmdList.append('echo ">'+'exon_'+m.group(4)+"\n"+'---" > '+kgPathDict[kgID]+'/'+spec+'/missingExon'+m.group(4)+'/cds.fa')
			elif m.group(9)=='mapped':
				cmdList.append('echo ">'+'exon_'+m.group(4)+"\n"+sequence+'" >>'+kgPathDict[kgID]+'/'+spec+'/cds.fa')
			else:
				regionList=m.group(9).split(';')
				regFileName=kgPathDict[kgID]+'/'+spec+'/region.list'
				numExt=0
				for region in regionList:
					numExt+=1
					chromosome,positions=region.split(':')
					start,end=positions[:-1].split('-')
					strand=positions[-1]
					if int(start)>int(end): # In case of complex regions no time is available for me to deal corretly with that so for now the sequence from ucsc is trusted... meaning that some insertion might be missed !
						#TODO deal better with those complex regions...
						mkdirp(kgPathDict[kgID]+'/'+spec+'/rearrangedExon'+m.group(4)+'_ext'+str(numExt))
						cmdList.append('echo ">'+'exon_'+m.group(4)+'_ext'+str(numExt)+"\n"+sequence+'" >>'+kgPathDict[kgID]+'/'+spec+'/rearrangedExon'+m.group(4)+'_ext'+str(numExt)+'/cds.fa')
					else:
						with open(regFileName+'.tmp','a') as regFile:
							regFile.write("\t".join([chromosome,start,end,strand,'exon_'+m.group(4)+'_ext'+str(numExt)])+"\n")
	if len(cmdList)!=0:
		getCDS(cmdList)
	rootedDir.finishSerializer("gwAlign_getCDS")
elif 'align' in modeList : # no align just after getCDS. need time to finish
	low_mem,res_mem,high_mem=args.mems.split(",")
	Nlim=int(args.batchLim)
	rootedDir.addCommand("alignCDS",'alignCDS.py')
	cmdList=list()
	serialBatch=rootedDir.addSerializer("gwAlign_alignCDS",Nlim,queue=args.queue,ncpus='1',mem=res_mem+"g",workdir=rootedDir.results)
	for key in kgPathDict.keys():
		if not (args.keep and os.path.isfile(kgPathDict[key]+'/ref_macse_AA.fasta')):
			if args.v : out_logger("align : missing alignment for "+key)
			optAlign={
				'-gene_dir':kgPathDict[key],
				'-ref':args.ref,
				'-virt_mem':low_mem,
				'-macse':args.macse,
				'-boost_mem':high_mem
			}
			cmdList.append(rootedDir.getCommand("alignCDS",kgPathDict[key],optAlign))
			rootedDir.serialize("gwAlign_alignCDS",cmdList)
			cmdList=list()
		rootedDir.finishSerializer("gwAlign_alignCDS")
# add elif to re-alignBigCDS with reporting ...
elif 'compute_TCS' in modeList:
	rootedDir.addCommand("t_coffee","t_coffee","t_coffee -version | sed 's/PROGRAM: T-COFFEE //g'")
	Nlim=int(args.batchLim)
	rootedDir.addSerializer("gwAlign_compute_TCS",Nlim,queue=args.queue,ncpus='2',mem="2gb",workdir=rootedDir.results)
	for key in kgPathDict.keys():
		mkdirp(kgPathDict[key]+'/tcs_eval')
		if not (args.keep and os.path.isfile(kgPathDict[key]+'/tcs_eval/translated_dna.score_ascii')):
			cmdList=["ln -sf "+kgPathDict[key]+"/ref_macse_NT.fasta "+kgPathDict[key]+'/tcs_eval/dna.fa']
			optTranslate={
				'-action':'+translate',
				'-in':'dna.fa',
				'-output':"fasta_aln"
			}
			posTranslate=["> translated_dna.fa"]
			cmdList.append(rootedDir.getCommand("t_coffee",options=optTranslate,subprogram="-other_pg seq_reformat",positionals=posTranslate,wd=kgPathDict[key]+'/tcs_eval'))
			optTCS={
				'-infile':'translated_dna.fa',
				'-evaluate':'',
				'-output':"score_ascii",
				'-master':args.ref,
				'-n_core':'2'
			}
			cmdList.append(rootedDir.getCommand("t_coffee",options=optTCS,wd=kgPathDict[key]+'/tcs_eval'))
			rootedDir.serialize("gwAlign_compute_TCS",cmdList)
	rootedDir.finishSerializer("gwAlign_compute_TCS")
elif 'blast_search' in modeList:
	if args.v : out_logger(" Entering mode blast_search")
	if args.blast_db == None:
		err_logger(" blast_search - argument -blast_db ")
		sys.exit(1)
	blast_db=os.path.abspath(args.blast_db)
	rootedDir.addCommand("blastp","blastp","blastp -version | grep 'blastp: ' | sed 's/blastp: //g'")
	rootedDir.addCommand("t_coffee","t_coffee","t_coffee -version | sed 's/PROGRAM: T-COFFEE //g'")
	Nlim=int(args.batchLim)
	rootedDir.addSerializer("gwAlign_blast_search",Nlim,queue=args.queue,ncpus='1',mem="2gb",workdir=rootedDir.results)
	for key in kgPathDict.keys():
		cmdList=list()
		optTranslate={
			'-action':'+translate',
			'-in':'ref.fa',
			'-output':"fasta_aln"
		}
		posTranslate=["> ref_tr.fa"]
		cmdList.append(rootedDir.getCommand("t_coffee",options=optTranslate,subprogram="-other_pg seq_reformat",positionals=posTranslate,wd=kgPathDict[key]))
		optBlast={
			'-query':'ref_tr.fa', 
			'-db':blast_db,
			'-use_sw_tback':'', 
			'-seg':'no',
			'-outfmt':'5',
			'-max_hsps_per_subject':'1', 
			'-max_target_seqs':'5'
		}
		posBlast=["> ref_blast.xml"]
		cmdList.append(rootedDir.getCommand("blastp",options=optBlast,positionals=posBlast,wd=kgPathDict[key]))
		rootedDir.serialize("gwAlign_blast_search",cmdList)
	rootedDir.finishSerializer("gwAlign_blast_search")



elif 'stat_filter' in modeList:

	if args.v : out_logger(" Entering mode stat_filter")
	tcs_min_cons,tcs_min_each=args.phase_filter.split(',')
	cpu=int(args.cpu)
	if args.focus!=None:
		focusList=args.focus.split(',')
	else:
		focusList=None
	selectRatio=float(args.ratio)
	def intscore(char):
		if char=='-':
			value=-1 # assign -1 score for gaps
		elif char=='#':
			value=-1
		else:
			value=int(char)
		return(value)

	class score:

		"""
		@summary: Parse t_coffee score files (ascii format)
		@ivar means : dictionary of mean score (ints) with seqname as keys
		@type means : dict(<string:int>)
		@ivar seqs : dictionary of seq score (ints) with seqname as keys
		@type seqs : dict(<string:int Array>)
		@ivar mean_cons: mean score for consensus
		@type mean_cons: int
		@ivar mean_seq: seq score for consensus
		@type mean_seq: int array
		"""

		def __init__(self,filename):
			import re
			re_score=re.compile('^SCORE=([\d]+)$')
			re_species=re.compile('^([\S]+)[\s]+:[\s]+([\d]+)$')
			re_seq=re.compile('^([\S]+)[\s]+([\-\d]+)$')
			self.means=dict() # dictionary of mean score (ints)
			self.seqs =dict() # dictionary of score ascii seq (int arrays)

			with open(filename,"r") as filescore:
				for line in filescore:
					line=line.rstrip()

					m=re_score.match(line)

					if m:
						self.score=intscore(m.group(1))
					else:
						m=re_species.match(line)
						if m:
							if m.group(1)!="cons":
								self.means[m.group(1)]=intscore(m.group(2))
								self.seqs[m.group(1)]=[]
							else:
								self.mean_cons=intscore(m.group(2))
								self.seq_cons=[]
						else:
							m=re_seq.match(line)
							if m:
								if m.group(1)!="cons":
									self.seqs[m.group(1)]+=[intscore(x) for x in list(m.group(2))]
								else:
									self.seq_cons+=[intscore(x) for x in list(m.group(2))]

	def tcs_filter_and_stats(in_gene_dir,in_ref=args.ref,in_name=args.name,in_tcs_min_cons=tcs_min_cons,in_tcs_min_each=tcs_min_each,in_out_stat="stat.txt",in_out_filtered="tcs_filtered_aln.fa",in_out_conserved_bed="codon_conserved.bed",in_out_exon="nucl_exon.bed",in_tcs_score="./tcs_eval/translated_dna.score_ascii",in_dna="./tcs_eval/dna.fa",in_focusList=focusList,in_ratio=selectRatio):
		
		"""
		parser.add_argument('-gene_dir', metavar='/path', required=False,default='.', help="gene ID directory")
		parser.add_argument('-ref', metavar='ref_species' , required=False, help="name of the reference species",default='hg19')
		parser.add_argument('-name', metavar='name', required=False,default="filtered_default", help="name of phasing-filtering")
		parser.add_argument('-tcs_min_cons', metavar='N', required=False,default=5, help="minimum of TCS quality 1-9 for consensus seq")
		parser.add_argument('-tcs_min_each', metavar='N', required=False,default=3, help="minimum of TCS quality 1-9 for each seq")
		parser.add_argument('-out_stat', metavar='name', required=False,default="stat.txt", help="name for stat file (one in parent and another in filtered directory)")
		parser.add_argument('-out_filtered', metavar='name', required=False,default="tcs_filtered_aln.fa", help="filtered fasta file")
		parser.add_argument('-out_conserved_bed', metavar='name', required=False,default="codon_conserved.bed", help="bed file for conserved regions")
		parser.add_argument('-out_exon', metavar='name', required=False,default="nucl_exon.bed", help="bed file for exonic intervall on ref sequence")
		parser.add_argument('-tcs_score', metavar='filename', required=False,default="./tcs_eval/translated_dna.score_ascii", help="score_ascii TCS file (based on translated dna) (t_coffee)")
		parser.add_argument('-dna', metavar='filename', required=False,default="./tcs_eval/dna.fa", help="score_ascii TCS file (t_coffee)")
		parser.add_argument('-v',required=False,action='store_true',help="verbose")
		"""
		import itertools
		import os
		import re
		import sys
		import traceback
		from Bio import SeqIO
		from Bio.Seq import Seq
		from upype import mkdirp
		from Bio.SubsMat import MatrixInfo
		allowedAA=['C','S','T','P','A','G','N','D','E','Q','H','R','K','M','I','L','V','F','Y','W']

		def score_matrix(pair):
			if pair not in MatrixInfo.blosum62:
				return MatrixInfo.blosum62[(tuple(reversed(pair)))]
			else:
				return MatrixInfo.blosum62[pair]

		def score_spec(nucl_ref,nucl_spec): # compute blossum62 score
			
			nucl_ref=nucl_ref.upper()
			nucl_spec=nucl_spec.upper()

			if not nucl_spec in allowedAA:
				minScore=100
				for AA in allowedAA:
					if score_matrix((nucl_ref,AA))<minScore:
						minScore=score_matrix((nucl_ref,AA))
				return(minScore)
			else:
				return(score_matrix((nucl_ref,nucl_spec)))

		def getAAFromCodingAlignedString(alignedString,posFirstBase):
			codon_spec=str(alignedString)[posFirstBase:posFirstBase+3]
			fs_gap=codon_spec.count("!")+codon_spec.count("-")
			if fs_gap > 0:
				aa_spec='-'
			else:
				aa_spec=Seq(codon_spec).translate().__str__().upper()
			return(aa_spec)

		os.chdir(in_gene_dir)
		mkdirp(in_name)
		re_title=re.compile('.*(tr|sp)\|(.*)\|(.*)_[\S]+ ([^=]*)( [^=]+=*)')
		err=""
		msg=""
		maxscore=None
		from Bio.Blast import NCBIXML
		result=open("ref_blast.xml","r")
		records= NCBIXML.parse(result)
		item=next(records)
		q_len=float(item.query_length)
		subject_id=None
		for alignment in item.alignments:
			s_len=float(alignment.length)
			for hsp in alignment.hsps:
				q_cov=float(hsp.align_length)/q_len
				s_cov=float(hsp.align_length)/s_len
				if q_cov>0.9 and s_cov>0.9:
					m=re_title.match(alignment.title)
					if not m:
						err="A blast database entry title {"+alignment.title+"} is not recognized for: "+in_gene_dir.rstrip('/').split('/')[-1]
						return({"out":msg,"err":err})
					#gene_name=m.group(3)
					#long_name=m.group(4)
					if hsp.score>maxscore or maxscore==None:
						subject_id=m.group(2)
						query_aln=hsp.query
						subject_aln=hsp.sbjct 
						maxscore=hsp.score
						subject_start=hsp.sbjct_start # all next 1-based
						subject_end=hsp.sbjct_end
						query_start=hsp.query_start
						query_end=hsp.query_end
					elif hsp.score==maxscore:
						db_type=m.group(1) # tr or sp
						if db_type=='sp':
							subject_id=m.group(2)
							query_aln=hsp.query
							subject_aln=hsp.sbjct
							subject_start=hsp.sbjct_start # all next 1-based
							subject_end=hsp.sbjct_end
							query_start=hsp.query_start
							query_end=hsp.query_end
		try:

			feat_dict={
				"INIT_MET":[],
				"DISULFID":[],
				"CROSSLNK":[],
				"ACT_SITE":[],
				"METAL":[],
				"BINDING":[]
			}

			if subject_id!=None:
				i=query_start # sould be first ref pos in the hit
				prot_uniprot2ref=[0]*subject_start # 1-based with a subject start set

				for j in range(0,len(query_aln)):
					aa_query=query_aln[j]
					aa_subject=subject_aln[j]
					if ((aa_subject!='-')and(aa_subject!='!')and(aa_subject!='?')):
						prot_uniprot2ref.append(i)
					if ((aa_query!='-')and(aa_query!='!')and(aa_query!='?')):
						i+=1

				import requests,json
				requestURL = "https://www.ebi.ac.uk/proteins/api/features/"+subject_id+"?types=INIT_MET%2CDISULFID%2CCROSSLNK%2CACT_SITE%2CMETAL%2CBINDING"
				r = requests.get(requestURL, headers={ "Accept" : "application/json"})
				if not r.ok:
					err="EBI API request failed for: "+in_gene_dir.rstrip('/').split('/')[-1]
					return({"out":msg,"err":err})
				uniprot_dict=json.loads(r.text)
				features = uniprot_dict["features"]
				uniprot_seq=uniprot_dict["sequence"] # to check wether aa is well aligned in human without fs

				#DISULFID COORD
				# begin of disulfid is the first cystein in a 1-based
				# end of disulfid is the second cystein in a 1-based

				#CROSSLNK COORD
				# IDEM for intrachain 
				# start=end in 1-based for inter-chain

				#Site COORD are 1-based

				for feat in features:
					try: 
						if int(feat["begin"]) >= subject_start and int(feat["begin"]) <= subject_end:
							feat_dict[feat["type"]].append(int(feat["begin"]))
							if feat["begin"]!=feat["end"] :
								if int(feat["end"]) >= subject_start and int(feat["end"]) <= subject_end :
									feat_dict[feat["type"]].append(int(feat["end"]))
					except Exception, e:
						pass

			out_filtered=in_name+"/"+in_out_filtered
			out_conserved_bed=in_name+"/"+in_out_conserved_bed
			out_exon=in_name+"/"+in_out_exon
			out_stat=in_out_stat
			out_stat_filtered=in_name+"/"+in_out_stat
			discard_path=in_name+"/discard.txt"
			
		
			alnScore=score(in_tcs_score)
			record_dict = SeqIO.to_dict(SeqIO.parse(in_dna, "fasta"))
			conserved_dict=dict()
			tcs_min_cons=int(in_tcs_min_cons)
			tcs_min_each=int(in_tcs_min_each)

			dna_ref2aln=[0] # 1-based
			i=1
			for nucl in str(record_dict[in_ref].seq):
				if ((nucl!='-')and(nucl!='!')and(nucl!='?')):
					dna_ref2aln.append(i)
				i+=1

			conservedArray=[]
			for i in range(0,len(alnScore.seq_cons)):
				if alnScore.seq_cons[i]>=tcs_min_cons:
					conservedArray+=[i]

			if alnScore.score==0:
				msg="stat_filter DISCARD see : "+os.path.abspath(discard_path)
				with open(discard_path,"w") as discard_file:
					discard_file.write("MSA have been discarded because the computed TCS score is 0. It could mean that only the reference is there.")
			elif float(len(alnScore.means.keys()))/float(len(record_dict.keys())) < in_ratio:
				msg="stat_filter DISCARD see : "+os.path.abspath(discard_path)
				with open(discard_path,"w") as discard_file:
					discard_file.write("MSA have been discarded because the ratio between conserved species (at least partially) and considered species is inferior to the specified ratio threshold : "+str(in_ratio))
			elif len(conservedArray)==0:
				msg="stat_filter DISCARD see : "+os.path.abspath(discard_path)
				with open(discard_path,"w") as discard_file:
					discard_file.write("MSA have been discarded because no columns were above the TCS threshold :"+str(tcs_min_cons))
			elif in_focusList!=None:
				total=float(len(in_focusList))
				conserved=0
				for species in in_focusList:
					if not species in record_dict.keys():
						raise ValueError('One of species name specified in -focus argument does not exists')
					elif species in alnScore.means.keys():
						conserved+=1
				if float(conserved)/total<in_ratio:
					msg="stat_filter DISCARD see : "+os.path.abspath(discard_path)
					with open(discard_path,"w") as discard_file:
						discard_file.write("MSA have been discarded because the ratio between conserved species (at least partially) and considered species specified in -focus argument is inferior to the specified ratio threshold : "+str(in_ratio))
			else:
				with open(out_stat,"w") as statFile:
					with open(out_stat_filtered,"w") as filteredStatFile:
						filteredStatFile.write("\t".join(["species","N","gap","fs"])+"\n")
						with open(out_filtered,"w") as outFile:
							raw_scores={
								"INIT_MET":dict(),
								"DISULFID":dict(),
								"CROSSLNK":dict(),
								"ACT_SITE":dict(),
								"METAL":dict(),
								"BINDING":dict()
							}

							header_feats="\t".join(raw_scores.keys())
							statFile.write("\t".join(["species","N","gap","fs","end_stop","before_end_stop","mean_score"])+"\t"+header_feats+"\n")
							for species in record_dict.keys():
								if not species in alnScore.means.keys():
									alnScore.means[species]=-1
									alnScore.seqs[species]=[-1]*len(alnScore.seq_cons)
								else:
									cod_seq=[str(record_dict[species].seq)[3*x:3*x+3] for x in range(0,len(str(record_dict[species].seq))/3)]
									conserved_dict[species]=[]
									outFile.write(">"+species+"\n")
									limLine=0
									N=str(record_dict[species].seq).count("N")
									N+=str(record_dict[species].seq).count("n")
									fs=str(record_dict[species].seq).count("!")
									gap=str(record_dict[species].seq).count("-")
									m_qual=alnScore.means[species]
									# TODO Compute feature scores !
									feat_line=""
									for f_type in feat_dict.keys():
										# initiate score to 0
										raw_scores[f_type][species]=0
										for aa_pos_uniprot_ref in sorted(set(feat_dict[f_type])):
											# dna_ref2aln prot_uniprot2ref
											aa_uniprot=uniprot_seq[aa_pos_uniprot_ref-1].upper()

											aa_pos_ref=prot_uniprot2ref[aa_pos_uniprot_ref-1]
											aln_cod_start=dna_ref2aln[(aa_pos_ref)*3]
											aa_ref=getAAFromCodingAlignedString(str(record_dict[in_ref].seq),aln_cod_start)

											if aa_ref==aa_uniprot:
												aa_spec=getAAFromCodingAlignedString(str(record_dict[species].seq),aln_cod_start)
												if aa_spec!='X':
													raw_scores[f_type][species]+=score_spec(aa_ref,aa_spec)

											### VERY USEFUL CTRL LINES

											if f_type=="DISULFID" and species==in_ref and str(record_dict[in_ref].seq).count('!')==0:
												tmp_err=""
												# Check if it is C ok ?
												if aa_uniprot!='C':
													tmp_err+="Uniprot sequence is not a C but a "+aa_uniprot+"\n"
												if aa_ref!='C' and aa_ref!='-' :
													tmp_err+="Ref seqeunce is not a C but a "+aa_ref+"\n"
												if tmp_err!="":
													tmp_err=" WARNING - DISULFID CHECKPOINT (uniprot id: "+subject_id+" [hit start="+str(subject_start)+"] at pos:"+str(aa_pos_uniprot_ref)+") failed:\n"+tmp_err
												err+=tmp_err
											###
										feat_line+="\t"+str(raw_scores[f_type][species])
									if re.match('TAG|TAA|TGA',list(reversed(list(itertools.dropwhile(lambda x: x == '---', reversed(cod_seq)))))[-1],re.IGNORECASE):
										end_stop=1
									else:
										end_stop=0
									before_end_stop=0
									oldcodon=None
									for codon in list(reversed(list(itertools.dropwhile(lambda x: x == '---', reversed(cod_seq)))))[0:-1]:
										if oldcodon==None:
											oldcodon=codon
										elif ( (oldcodon+codon).count('!')==3 and re.match('TAG|TAA|TGA',re.sub('!','',oldcodon+codon),re.IGNORECASE)):
											before_end_stop+=1
											oldcodon=codon
									statFile.write("\t".join([species,str(N),str(gap),str(fs),str(end_stop),str(before_end_stop),str(m_qual)])+feat_line+"\n")
									for i in range(0,len(cod_seq)):
										if i in conservedArray: # if the consensus is not of a good quality ==> clipping
											limLine+=1
											if alnScore.seqs[species][i]>=tcs_min_each or alnScore.seqs[species][i]==-1:
												outFile.write(re.sub('!','-',cod_seq[i]))
												conserved_dict[species]+=[cod_seq[i]]
											else :
												outFile.write('NNN')
												conserved_dict[species]+=['NNN'] # hardmasking for badly aligned sequences
										if limLine==20:
											outFile.write("\n")
											limLine=0
									if limLine!=0 : outFile.write("\n")
									N=''.join(conserved_dict[species]).count("N")
									N+=''.join(conserved_dict[species]).count("n")
									fs=''.join(conserved_dict[species]).count("!")
									gap=''.join(conserved_dict[species]).count("-")
									filteredStatFile.write("\t".join([species,str(N),str(gap),str(fs)])+"\n")
				msg="stat_filter FINISHED for :"+in_gene_dir.rstrip('/').split('/')[-1]
				newInterval=True
				start=0
				line="# Conserved codons (in codon/amino acid index) with aligned sequence as reference (counting gaps)"
				with open(out_conserved_bed,"w") as outFile:
					for i in conservedArray:
						if newInterval:
							outFile.write(line+"\n")
							old_i=i
							newInterval=False
						else:
							if i-1!=old_i:
								newInterval=True
								line=in_ref+"\t"+str(start)+"\t"+str(old_i+1) # 0-based IN OUT
								start=i
							else:
								old_i=i
					if not newInterval:
						line=in_ref+"\t"+str(start)+"\t"+str(old_i+1) # 0-based IN OUT
					outFile.write(line+"\n")

				i=0
				line="# exons (in nucleotid index) with unaligned sequence as reference (not counting gaps)"
				newInterval=True
				start=0

				with open(out_exon,"w") as outFile:
					with open("exons.pos") as exonFile:
						for exonline in exonFile.readlines():
							exonline=exonline.rstrip()
							if newInterval:
								outFile.write(line+"\n")
								old_i=i
								numExon=exonline
								newInterval=False
							else:
								if exonline!=numExon:
									newInterval=True
									line=in_ref+"\t"+str(start)+"\t"+str(old_i+1) # 0-based IN OUT
									start=i
								else:
									old_i=i
							i+=1
					if not newInterval:
						line=in_ref+"\t"+str(start)+"\t"+str(old_i+1) # 0-based IN OUT
					outFile.write(line+"\n")
		except Exception, e:
			exc_type, exc_value, exc_tb = sys.exc_info()
			err="\n".join(traceback.format_exception(exc_type, exc_value, exc_tb))
		if err!="":
			err="stat_filter: ERROR failed job for :"+in_gene_dir.rstrip('/').split('/')[-1]+"\n"+err
		return({"out":msg,"err":err})

	from multiprocessing import Pool
	
	pool=Pool(processes=cpu)
	jobs=[]
	for kgID in kgPathDict.keys():
		if args.v: 
			out_logger("stat_filter: initiate job for "+kgID+" in "+kgPathDict[kgID])
		jobs.append(pool.apply_async(tcs_filter_and_stats,args=(kgPathDict[kgID],)))

	pool.close()

	while len(jobs)!=0:
		for i in range(0,min(cpu+1,len(jobs))):
			if jobs[i].ready():
				outjob=jobs[i].get(5)
				if outjob["err"]=="":
					if args.v : out_logger(outjob["out"])
				else:
					err_logger(outjob["err"])
					#sys.exit(1)
				del jobs[i]
				if args.v : out_logger("stat_filter -- "+str(len(jobs))+" jobs left")
				break

	
	pool.join()	
	
### Annotation
if 'annotate' in modeList:
	allSymbol=dict()
	iterNoName=1
	
	cnx = mysql.connector.connect(user='genome',host=args.host,port=args.port,database='hg19')
	for key in kgPathDict.keys():
		gene_name=Get_annotation(key)
	cnx.close()

	with open(rootedDir.reports+'/all_withCounts.txt','w') as logFile:
		for key in allSymbol:
			logFile.write(key+"\t"+str(allSymbol[key])+"\n")
	with open(rootedDir.reports+'/duplicate.txt','w') as logFile:
		for key in allSymbol:
			if allSymbol[key]>0:
				logFile.write(key+"\n")
	submit('cd '+rootedDir.path+' ; for kgID in $(ls results) ; do echo -e "$kgID\t$(cat results/$kgID/annotation/consName.txt)" ; done > reports/consName.tab')
	submit('cd '+rootedDir.reports+' ; for path in $(awk \'{ print $2 }\' kgPathDictFile.tab); do cat $path/annotation/consName.txt | sed -r "s/$/\t$(echo $path | sed \'s/\//\\\//g\')\n/g" ; done > consNameDict.tab')

	dirList=os.listdir(rootedDir.results)
	goDict=dict()
	goList=list()
	headGO=['goId','name','type','genes']
	keggDict=dict()
	keggList=list()
	headKegg=['mapId','name','genes']
	keggFileName=rootedDir.reports+'/kegg.tab'
	goFileName=rootedDir.reports+'/go.tab'

	for dirName in dirList:
		with open(rootedDir.results+'/'+dirName+'/annotation/consName.txt') as consNameFile:
			gene=consNameFile.readline().rstrip()
		File=open(rootedDir.results+'/'+dirName+'/annotation/go.tab','r')
		File.readline()
		for line in File.readlines():
			line=line.rstrip()
			lineList=line.split("\t")
			goId=lineList[1]
			name=lineList[2]
			gotype=lineList[3]
			if goId in goDict.keys():
				goDict[goId].append(gene)
			else:
				goDict[goId]=[gene]
				goList.append([goId,name,gotype])
		File.close()
		File=open(rootedDir.results+'/'+dirName+'/annotation/kegg.tab','r')
		File.readline()
		for line in File.readlines():
			line=line.rstrip()
			lineList=line.split("\t")
			mapId=lineList[1]
			name=lineList[2]
			if mapId in keggDict.keys():
				keggDict[mapId].append(gene)
			else:
				keggList.append([mapId,name])
				keggDict[mapId]=[gene]
		File.close()
	keggFile=open(keggFileName,'w')
	keggFile.write("\t".join(headKegg)+"\n")
	goFile=open(goFileName,'w')
	goFile.write("\t".join(headGO)+"\n")
	for lineList in keggList:
		mapId=lineList[0]
		keggFile.write("\t".join(lineList)+"\t"+','.join(keggDict[mapId])+"\n")
	for lineList in goList:
		goId=lineList[0]
		goFile.write("\t".join(lineList)+"\t"+','.join(goDict[goId])+"\n")
	goFile.close()
	keggFile.close()

## end of annotation

saveRoot(rootedDir)
sys.exit(0)


