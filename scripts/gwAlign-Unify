#!/usr/bin/env python
import argparse
version='SEDMATCHGITVERSION'
year=2018
author='Julien Fouret'
contact='julien@fouret.me'

##parse argument

parser = argparse.ArgumentParser(description=
"""
Genome-Wide Alignments - Unify

From Exons to well-aligned and filtered CDS
                        ---------------
Steps automatized on HPC infrastructure with upype python package
For more informations:
https://fouret.me/gitea/jfouret/upype
                        ---------------
-------------------------------------------------------------------
mode:create         First create a file architecture to extract CDS 
                    from a database of MSA (Multiple Sequence 
                    Alignment) at exon-level formatted like UCSC.
-------------------------------------------------------------------
mode:getCDS         Then reassemble all exons into one CDS avoiding 
                    reproducing mistakes in the parent file. 
-------------------------------------------------------------------
mode:annotate       Optionally get the corresponding name (HUGO) of
                    the gene with the accession ids (UCSC, NCBI and 
                    Uniprot) and add kegg, go and uniprot 
                    annotations based on a UCSC-like  mySQL 
                    database. 
-------------------------------------------------------------------
mode:align          re-align all CDS with a coding-aware method 
                    (macse). 
-------------------------------------------------------------------
mode:compute_TCS    Compute TCS (Transitive consistency score) with
                    t_coffee for each alignment
-------------------------------------------------------------------
mode:stat_filter    Use TCS score to filter MSA and to produce 
                    statistics on sequence qualities by species.
-------------------------------------------------------------------
"""
,epilog="Version : "+str(version)+"\n"+str(year)+"\nAuthor : "+author+" for more informations or enquiries please contact "+contact,formatter_class=argparse.RawTextHelpFormatter)

parser.add_argument('-mode', metavar='keyword', required=True, help="Only one by one")
parser.add_argument('-al', metavar='/path', required=False, help="[create/getCDS] alignment file in fasta format ")
parser.add_argument('-out', metavar='/path', required=True, help="[all] folder in which to write all computed alignments ")
parser.add_argument('-v',action='store_true', help="[all] verbose")
parser.add_argument('-debug',action='store_true', help="[all] more verbose")
parser.add_argument('-batchLim', metavar='N', default='10', help="[getCDS/align/compute_TCS] Number of operation per batch")
parser.add_argument('-mems', metavar='lm,rm,hm', default='10,3,256', help="""[align] Precise memory config in GB for alignments:
>lm is the basic parameter to use first for alignments
>rs is what jobs will reserve 
>hm is an increased amount of memory for failures
default='10,3,256'""")
parser.add_argument('-spec', metavar='/path' , required=False, help="""[getCDS] tab file with assemblies info (name:path) 
(with .dict and .fai)""")
parser.add_argument('-host', metavar='IP' , required=False, help="""[annotate] mysql host for ucsc database 
(default='10.0.0.200')""",default='10.0.0.200')
parser.add_argument('-port', metavar='PORT' , required=False, help="""[annotate]mysql port for ucsc database
(default='3306')""",default='3306')
parser.add_argument('-ref', metavar='name', required=False, help="""[create/align/compute_TCS] reference species for multiple alignment 
mode annotate hase been design and tested only for hg19
(default='hg19')""",default='hg19')
parser.add_argument('-phase_filter', metavar='N,N', required=False, help="""[stat_filter] Min TCS score with:
1: consensus sequence
2: each sequences 
(default='5,3')""",default='5,3')
parser.add_argument('-queue', metavar='queue', required=False, help="""[all] queue for PBS 
(default='SEDMATCHQUEUE')""",default='SEDMATCHQUEUE')
parser.add_argument('-picard', metavar='/path', required=False, help="""[getCDS] picard jar path 
(default='SEDMATCHPICARD')""",default='SEDMATCHPICARD')
parser.add_argument('-macse', metavar='/path', required=False, help="""[align] macse jar path 
(default='SEDMATCHMACSE')""",default='SEDMATCHMACSE')

args=parser.parse_args()

import os
import sys
import mysql.connector
from Bio import SeqIO
from Bio.Seq import Seq
import re
from upype import *

##define function
def writedb(query,file_name,header):
	global cnx
	global args
	if args.v:
		print query
		print("\n")
	cursor=cnx.cursor()
	cursor.execute(query)
	rows=cursor.fetchall()
	tab_file=open(file_name,"w")
	tab_file.write("\t".join(header))
	for row in rows:
		tab_file.write("\n")
		tab_file.write("\t".join(map(str,row)))
	tab_file.close()
	cursor.close()
def Get_annotation(ID):
	"""
	@summary: Take a kgID as input to search the ucsc database for name, cross reference ids and functional information for KEGG and GO annotation spaces. All information are written in other files.
	@param ID: a kgID, kg stand for knowGene
	@type ID: str  
	"""
	global args
	global iterNoName
	global allSymbol
	field='kgID'
	#global ID_type
	query_where='WHERE (ref.'+field+'="'+ID+'");'
	query_name=['SELECT hgnc.symbol,ref.kgID,ref.spID,ref.refseq,ref.geneSymbol']
	query_name.append('FROM kgXref ref')
	query_name.append('LEFT JOIN proteinDB.hgncXref hgnc ON ( hgnc.uniProt=ref.spID ) AND (hgnc.refSeq=ref.refseq)')
	query_name.append(query_where)

	query_alias=['SELECT ref.alias']
	query_alias.append('FROM kgAlias ref')
	query_alias.append(query_where)

	query_uniprot=['SELECT ref.geneSymbol,feats.start,feats.end,class.val AS `class`,type.val AS `type`,feats.softEndBits AS `OutOfRange`']
	query_uniprot.append('FROM uniProt.feature feats')
	query_uniprot.append('LEFT JOIN kgXref ref ON (ref.spID=feats.acc)')
	query_uniprot.append('LEFT JOIN uniProt.featureClass class ON (class.id=feats.featureClass)')
	query_uniprot.append('LEFT JOIN uniProt.featureType type ON (type.id=feats.featureType)')
	query_uniprot.append(query_where)

	query_kegg=['SELECT ref.geneSymbol,kegg.mapID,des.description']
	query_kegg.append('FROM keggPathway kegg')
	query_kegg.append('LEFT JOIN kgXref ref ON (ref.kgID=kegg.kgID)')
	query_kegg.append('LEFT JOIN keggMapDesc des ON (kegg.mapID=des.mapID)')
	query_kegg.append(query_where)

	query_go=['SELECT ref.geneSymbol, goa.goId, goterm.name, goterm.term_type']
	query_go.append('FROM go.goaPart goa')
	query_go.append('LEFT JOIN kgXref ref ON (ref.spID=goa.dbObjectId)')
	query_go.append('LEFT JOIN go.term goterm ON (goa.goId=goterm.acc)')
	query_go.append(query_where)

	annotPath=rootedDir.results+"/"+ID+"/annotation/"
	mkdirp(annotPath)
	writedb(" ".join(query_name),annotPath+"/name.tab",['geneSymbol','kgID','uniprot','refSeq','kgName'])
	name_file=open(annotPath+"/name.tab","r")
	name_file_line1=name_file.readlines()[1]
	gene_symbol=name_file_line1.split("\t")[0]
	hgnc=name_file_line1.split("\t")[0]
	spID=name_file_line1.split("\t")[2]
	refseq=name_file_line1.split("\t")[3]
	kgname=name_file_line1.split("\t")[4]
	gene_symbol=hgnc
	if (hgnc=='None' or hgnc=='') or (spID=='' and refseq=='') :
		gene_symbol='kg_'+kgname
	elif kgname=='None' or kgname=='':
		gene_symbol='sp_'+spID
	elif spID=='None' or spID=='':
		gene_symbol='rs_'+refseq
	elif refseq=='None' or refseq=='':
		gene_symbol='NoID_'+str(iterNoName)
		iterNoName+=1
	gene_symbol=renameFileName(gene_symbol)
	if (gene_symbol in allSymbol.keys()):
		allSymbol[gene_symbol]+=1
		gene_symbol='dup'+str(allSymbol[gene_symbol])+'_'+gene_symbol
	else:
		allSymbol[gene_symbol]=0
	name_file.close()
	with  open(annotPath+"/consName.txt",'w') as consNameFIle:
		consNameFIle.write(gene_symbol)
	#os.system('mv '+rootedDir.results+"/"+ID+".name.tab"+' '+rootedDir.results+"/"+gene_symbol+'-'+ID+".name.tab")
	writedb(" ".join(query_uniprot),annotPath+"/uniprot.tab",['geneSymbol','start','end','class','type','OutOfRange'])
	writedb(" ".join(query_kegg),annotPath+"/kegg.tab",['geneSymbol','mapID','description'])
	writedb(" ".join(query_go),annotPath+"/go.tab",['geneSymbol','goId','name','type'])
	writedb(" ".join(query_alias),annotPath+"/alias.txt",['alias'])
	return gene_symbol
#start program

#parse args
modeList=args.mode.split(',')

### Create directories and listing...
kgPathDict=dict()
if 'create' in modeList:
	alnFileName=os.path.abspath(args.al)
	rootedDir=RootDir(args.out,batch=True)
	rootedDir.logs.writeArgs(args)
	# iter and create and print ref.fa
	rehg19Exon1=re.compile('^>(uc\w*)(\.\d*)_'+args.ref+'_1_(\d*) (\d*) (\d*) (\d*)\s?(\w+:\d+-\d+[+-]{1};?)*$')
	exons_file=open(alnFileName,'r')
	for line in exons_file.readlines():
		m=rehg19Exon1.match(line.rstrip())
		if m:
			if args.v:
				print('##Create_dirs## '+line)
			kgID=m.group(1)+m.group(2)
			path=rootedDir.results+"/"+kgID
			kgPathDict[kgID]=path
			mkdirp(path)
	with open(rootedDir.reports+'/kgPathDictFile.tab','w') as kgPathDictFile:
		for key in kgPathDict.keys():
			kgPathDictFile.write(key+"\t"+kgPathDict[key]+"\n")
	saveRoot(rootedDir)
else:
	if args.debug:
		rootedDir=RootDir(args.out,batch=True)
		rootedDir.logs.writeArgs(args)
	else:
		rootedDir=loadRoot(args.out)
	kgPathDict=dict()
	with open(rootedDir.reports+'/kgPathDictFile.tab','r') as kgPathDictFile:
		for line in kgPathDictFile.readlines():
			key,value=line.rstrip().split("\t")
			kgPathDict[key]=value

# get CDS
#########################
###### Explanation ######
# All this workflow is based on a file from ucsc which contains all sequences matching any hg19 knowngene \
# However this file has a major problem:  no gap have been considered in hg19 sequences. Meaning that     \
# only deletion are considered in other species and insertion are not reported in this files.
# Fortunately there is one fasta entry per exon per species and the entry name gives the genomic location \
# of the each exon is the corresponding species' genome. 
# The next lines of codes focuses on parsing this files for genomic coordinates aiming at using Picard    \ 
# to extract sequences from the genomes.
#
# In addition to this first issue, 5 cases are possible with those coordinates
# 	1- The coordinates are complete, fragmented or not, and consistent meaning that start<end thus        \
# 	those locations are appended to a bed file that will finally used to produce a cds.fa file.
# 	2- The coordinates are missing then a folder is created with '---' sequence in a cds.fa file
# 	3- The coordinates are not consistent. start>end ... well I suppose the outer sequences should be     \
# 	considered. Due to lack of time this feature has not been implemented yet and original sequences are  \
# 	added in a cds.fa file (with potential insertions missing...)
# 	4- The species is te reference (hg19) thus original sequences from input file have been used
# 	5- In place of the genomic location the mention "mapped" is present. This means that exons have been  \
# 	assembled using mapping then platypus thus orginals sequences have been used.

if 'getCDS' in modeList:
	alnFileName=os.path.abspath(args.al)
	specDict=dict()
	with open(args.spec,'r') as species_file:
		for line in species_file.readlines():
			if line.rstrip()!='':
				if args.v:
					print('##Parsing species## '+line)
				specie,assembly=line.rstrip().split("\t")
				specDict[specie]=assembly
	rootedDir.addCommand('java','java -XX:ParallelGCThreads=1','java -version 2>&1 | xargs')

	picard_cmd=rootedDir.getCommand("java",options={'-jar':args.picard})
	rootedDir.addCommand("picard",picard_cmd,picard_cmd+' CheckFingerprint --version 2>&1 | sed \'s/(.*$//g\'')
	reID=re.compile('^(uc\w*)(\.\d*)_(\w*)_(\d*)_(\d*) (\d*) (\d*) (\d*)\s*(\w+.*)?$')
	fasta_sequences = SeqIO.parse(open(alnFileName),'fasta')

	cmdList=list()
	iterBatch=1
	Nlim=int(args.batchLim)

	serialBatch=rootedDir.addSerializer("gwAlign_getCDS",Nlim,queue=args.queue,ncpus='1',mem="6gb",workdir=rootedDir.results,verbose=args.v)

	init=True

	# step 1 - iteration over exon file
	for fasta in fasta_sequences:
		# step 1.2 - get name and sequence
		name, sequence = fasta.description, str(fasta.seq)
		m=reID.match(name)
		if m and (m.group(3) in specDict.keys()):
			spec=m.group(3)
			kgID=m.group(1)+m.group(2)
			if init:
				oldkgID=kgID
				init=False
			if kgID!=oldkgID:
				if args.v:
					print("Getting CDS of "+oldkgID)
				oldkgPath=kgPathDict[oldkgID]
				for key in specDict.keys():
					oldRegFileName=oldkgPath+'/'+key+'/region.list'
					#TODO si regionlist exist
					if args.v:
						print("  -- Focus on species: "+key)
					if os.path.isfile(oldRegFileName+'.tmp'):
						getRefOpt={
							'R':specDict[key],
							'O':oldkgPath+'/'+key+'/cds.fa',
							'INTERVAL_LIST':oldRegFileName,
							'VERBOSITY':'ERROR',
							'QUIET':'true'
						}
						cmdList.append('cat '+specDict[key].rstrip('fa').rstrip('fasta').rstrip('fna')+'dict '+oldRegFileName+'.tmp'+' > '+oldRegFileName+' ; rm '+oldRegFileName+'.tmp')
						cmdList.append(rootedDir.getCommand("picard",options=getRefOpt,subprogram='ExtractSequences',sep='='))
						cmdList.append('rm '+oldRegFileName)
				if args.v: print("#INFO: Getting CDS of "+oldkgID)
				rootedDir.serialize("gwAlign_getCDS",cmdList)
				cmdList=list()
				oldkgID=kgID
			mkdirp(kgPathDict[kgID]+'/'+spec)
			if m.group(9)==None:
				mkdirp(kgPathDict[kgID]+'/'+spec+'/missingExon'+m.group(4))
				cmdList.append('echo ">'+'exon_'+m.group(4)+"\n"+'---" > '+kgPathDict[kgID]+'/'+spec+'/missingExon'+m.group(4)+'/cds.fa')
			elif m.group(9)=='mapped':
				cmdList.append('echo ">'+'exon_'+m.group(4)+"\n"+sequence+'" >>'+kgPathDict[kgID]+'/'+spec+'/cds.fa')
			else:
				regionList=m.group(9).split(';')
				regFileName=kgPathDict[kgID]+'/'+spec+'/region.list'
				numExt=0
				for region in regionList:
					numExt+=1
					chromosome,positions=region.split(':')
					start,end=positions[:-1].split('-')
					strand=positions[-1]
					if int(start)>int(end): # In case of complex regions no time is available for me to deal corretly with that so for now the sequence from ucsc is trusted... meaning that some insertion might be missed !
						#TODO deal better with those complex regions...
						mkdirp(kgPathDict[kgID]+'/'+spec+'/rearrangedExon'+m.group(4)+'_ext'+str(numExt))
						cmdList.append('echo ">'+'exon_'+m.group(4)+'_ext'+str(numExt)+"\n"+sequence+'" >>'+kgPathDict[kgID]+'/'+spec+'/rearrangedExon'+m.group(4)+'_ext'+str(numExt)+'/cds.fa')
					else:
						with open(regFileName+'.tmp','a') as regFile:
							regFile.write("\t".join([chromosome,start,end,strand,'exon_'+m.group(4)+'_ext'+str(numExt)])+"\n")
	rootedDir.finishSerializer("gwAlign_getCDS")
elif 'align' in modeList : # no align just after getCDS. need time to finish
	low_mem,res_mem,high_mem=args.mems.split(",")
	Nlim=int(args.batchLim)
	rootedDir.addCommand("alignCDS",'alignCDS.py')
	cmdList=list()
	serialBatch=rootedDir.addSerializer("gwAlign_alignCDS",Nlim,queue=args.queue,ncpus='1',mem=res_mem+"g",workdir=rootedDir.results)
	for key in kgPathDict.keys():
		optAlign={
			'-gene_dir':kgPathDict[key],
			'-ref':args.ref,
			'-virt_mem':low_mem,
			'-macse':args.macse,
			'-boost_mem':high_mem
		}
		cmdList.append(rootedDir.getCommand("alignCDS",kgPathDict[key],optAlign))
		rootedDir.serialize("gwAlign_alignCDS",cmdList)
		cmdList=list()
	rootedDir.finishSerializer("gwAlign_alignCDS")
# add elif to re-alignBigCDS with reporting ...
elif 'compute_TCS' in modeList:
	rootedDir.addCommand("t_coffee","t_coffee","t_coffee -version | sed 's/PROGRAM: T-COFFEE //g'")
	Nlim=int(args.batchLim)
	rootedDir.addSerializer("gwAlign_compute_TCS",Nlim,queue=args.queue,ncpus='2',mem="2gb",workdir=rootedDir.results)
	for key in kgPathDict.keys():
		mkdirp(kgPathDict[key]+'/tcs_eval')
		cmdList=["ln -sf "+kgPathDict[key]+"/ref_macse_NT.fasta "+kgPathDict[key]+'/tcs_eval/dna.fa']
		optTranslate={
			'-action':'+translate',
			'-in':'dna.fa',
			'-output':"fasta_aln"
		}
		posTranslate=["> translated_dna.fa"]
		cmdList.append(rootedDir.getCommand("t_coffee",options=optTranslate,subprogram="-other_pg seq_reformat",positionals=posTranslate,wd=kgPathDict[key]+'/tcs_eval'))
		optTCS={
			'-infile':'translated_dna.fa',
			'-evaluate':'',
			'-output':"score_ascii",
			'-master':args.ref,
			'-n_core':'2'
		}
		cmdList.append(rootedDir.getCommand("t_coffee",options=optTCS,wd=kgPathDict[key]+'/tcs_eval'))
		rootedDir.serialize("gwAlign_compute_TCS",cmdList)
	rootedDir.finishSerializer("gwAlign_compute_TCS")
elif 'stat_filter' in modeList:
	tcs_min_cons,tcs_min_each=args.phase_filter.split(',')
	def intscore(char):
		if char=='-':
			value=9 # assign max score for gaps ... to keep them
		else:
			value=int(char)
		return(value)

	class score:

		"""
		@summary: Parse t_coffee score files (ascii format)
		@ivar means : dictionary of mean score (ints) with seqname as keys
		@type means : dict(<string:int>)
		@ivar seqs : dictionary of seq score (ints) with seqname as keys
		@type seqs : dict(<string:int Array>)
		@ivar mean_cons: mean score for consensus
		@type mean_cons: int
		@ivar mean_seq: seq score for consensus
		@type mean_seq: int array
		"""

		def __init__(self,filename):
			import re
			re_score=re.compile('^SCORE=([\d]+)$')
			re_species=re.compile('^([\S]+)[\s]+:[\s]+([\d]+)$')
			re_seq=re.compile('^([\S]+)[\s]+([\-\d]+)$')
			self.means=dict() # dictionary of mean score (ints)
			self.seqs =dict() # dictionary of score ascii seq (int arrays)

			with open(filename,"r") as filescore:
				for line in filescore:
					line=line.rstrip()

					m=re_score.match(line)

					if m:
						self.score=intscore(m.group(1))
					else:
						m=re_species.match(line)
						if m:
							if m.group(1)!="cons":
								self.means[m.group(1)]=intscore(m.group(2))
								self.seqs[m.group(1)]=[]
							else:
								self.mean_cons=intscore(m.group(2))
								self.seq_cons=[]
						else:
							m=re_seq.match(line)
							if m:
								if m.group(1)!="cons":
									self.seqs[m.group(1)]+=[intscore(x) for x in list(m.group(2))]
								else:
									self.seq_cons+=[intscore(x) for x in list(m.group(2))]

	def tcs_filter_and_stats(in_gene_dir,in_ref,in_name="filtered_default",in_tcs_min_cons='5',in_tcs_min_each=3,in_out_stat="stat.txt",in_out_filtered="tcs_filtered_aln.fa",in_out_conserved="codon_conserved.bed",in_out_exon="nucl_exon.bed",in_tcs_score="./tcs_eval/translated_dna.score_ascii",in_dna="./tcs_eval/dna.fa"):
		
		"""
		parser.add_argument('-gene_dir', metavar='/path', required=False,default='.', help="gene ID directory")
		parser.add_argument('-ref', metavar='ref_species' , required=False, help="name of the reference species",default='hg19')
		parser.add_argument('-name', metavar='name', required=False,default="filtered_default", help="name of phasing-filtering")
		parser.add_argument('-tcs_min_cons', metavar='N', required=False,default=5, help="minimum of TCS quality 1-9 for consensus seq")
		parser.add_argument('-tcs_min_each', metavar='N', required=False,default=3, help="minimum of TCS quality 1-9 for each seq")
		parser.add_argument('-out_stat', metavar='name', required=False,default="stat.txt", help="name for stat file (one in parent and another in filtered directory)")
		parser.add_argument('-out_filtered', metavar='name', required=False,default="tcs_filtered_aln.fa", help="filtered fasta file")
		parser.add_argument('-out_conserved_bed', metavar='name', required=False,default="codon_conserved.bed", help="bed file for conserved regions")
		parser.add_argument('-out_exon', metavar='name', required=False,default="nucl_exon.bed", help="bed file for exonic intervall on ref sequence")
		parser.add_argument('-tcs_score', metavar='filename', required=False,default="./tcs_eval/translated_dna.score_ascii", help="score_ascii TCS file (based on translated dna) (t_coffee)")
		parser.add_argument('-dna', metavar='filename', required=False,default="./tcs_eval/dna.fa", help="score_ascii TCS file (t_coffee)")
		parser.add_argument('-v',required=False,action='store_true',help="verbose")
		"""
		import os
		import re
		import sys
		from Bio import SeqIO
		from upype import mkdirp

		mkdirp(in_name)

		out_filtered=in_name+"/"+in_out_filtered
		out_conserved_bed=in_name+"/"+in_out_conserved_bed
		out_exon=in_name+"/"+in_out_exon
		out_stat=in_out_stat
		out_stat_filtered=in_name+"/"+in_out_stat

		os.chdir(in_gene_dir)

		alnScore=score(in_tcs_score)
		record_dict = SeqIO.to_dict(SeqIO.parse(in_dna, "fasta"))
		conserved_dict=dict()
		tcs_min_cons=int(in_tcs_min_cons)
		tcs_min_each=int(in_tcs_min_each)

		conservedArray=[]
		for i in range(0,len(alnScore.seq_cons)):
			if alnScore.seq_cons[i]>=tcs_min_cons:
				conservedArray+=[i]

		with open(out_stat,"w") as statFile:
			statFile.write("\t".join(["species","N","gap","fs","end_stop","before_end_stop","mean_score"])+"\n")
			with open(out_stat_filtered,"w") as filteredStatFile:
				filteredStatFile.write("\t".join(["species","N","gap","fs","end_stop","before_end_stop"])+"\n")
				with open(out_filtered,"w") as outFile:
					for species in record_dict.keys():
						cod_seq=[str(record_dict[species].seq)[3*x:3*x+3] for x in range(0,len(str(record_dict[species].seq))/3)]
						conserved_dict[species]=[]
						outFile.write(">"+species+"\n")
						limLine=0
						N=str(record_dict[species].seq).count("N")
						N+=str(record_dict[species].seq).count("n")
						fs=str(record_dict[species].seq).count("!")
						gap=str(record_dict[species].seq).count("-")
						m_qual=alnScore.means[species]
						if re.match('TAG|TAA|TGA',cod_seq[-1],re.IGNORECASE):
							end_stop=1
						else:
							end_stop=0
						before_end_stop=0
						for codon in cod_seq[0:-1]:
							if re.match('TAG|TAA|TGA',codon,re.IGNORECASE):
								before_end_stop+=1
						statFile.write("\t".join([species,str(N),str(gap),str(fs),str(end_stop),str(before_end_stop),str(m_qual)])+"\n")
						for i in range(0,len(cod_seq)):
							if i in conservedArray: # if the consensus is not of a good quality ==> clipping
								limLine+=1
								if alnScore.seqs[species][i]>=tcs_min_each:
									outFile.write(re.sub('!','-',cod_seq[i]))
									conserved_dict[species]+=[cod_seq[i]]
								else :
									outFile.write('NNN')
									conserved_dict[species]+=['NNN'] # hardmasking for badly aligned sequences
							if limLine==20:
								outFile.write("\n")
								limLine=0
						if limLine!=0 : outFile.write("\n")

						N=''.join(conserved_dict[species]).count("N")
						N+=''.join(conserved_dict[species]).count("n")
						fs=''.join(conserved_dict[species]).count("!")
						gap=''.join(conserved_dict[species]).count("-")
						if re.match('TAG|TAA|TGA',conserved_dict[species][-1],re.IGNORECASE):
							end_stop=1
						else:
							end_stop=0
						before_end_stop=0
						for codon in conserved_dict[species][0:-1]:
							if re.match('TAG|TAA|TGA',codon,re.IGNORECASE):
								before_end_stop+=1
						filteredStatFile.write("\t".join([species,str(N),str(gap),str(fs),str(end_stop),str(before_end_stop)])+"\n")


		newInterval=True
		start=0
		line="# Conserved codons (in codon/amino acid index) with aligned sequence as reference (counting gaps)"
		with open(out_conserved_bed,"w") as outFile:
			for i in conservedArray:
				if newInterval:
					outFile.write(line+"\n")
					old_i=i
					newInterval=False
				else:
					if i-1!=old_i:
						newInterval=True
						line=in_ref+"\t"+str(start)+"\t"+str(old_i+1) # 0-based IN OUT
						start=i
					else:
						old_i=i
			if not newInterval:
				line=in_ref+"\t"+str(start)+"\t"+str(old_i+1) # 0-based IN OUT
			outFile.write(line+"\n")

		i=0
		line="# exons (in nucleotid index) with unaligned sequence as reference (not counting gaps)"
		newInterval=True
		start=0

		with open(out_exon,"w") as outFile:
			with open("exons.pos") as exonFile:
				for exonline in exonFile.readlines():
					exonline=exonline.rstrip()
					if newInterval:
						outFile.write(line+"\n")
						old_i=i
						numExon=exonline
						newInterval=False
					else:
						if exonline!=numExon:
							newInterval=True
							line=in_ref+"\t"+str(start)+"\t"+str(old_i+1) # 0-based IN OUT
							start=i
						else:
							old_i=i
					i+=1
			if not newInterval:
				line=in_ref+"\t"+str(start)+"\t"+str(old_i+1) # 0-based IN OUT
			outFile.write(line+"\n")

	with open(args.phase_filter,'r') as configFile:
		filterDict=dict()
		for line in configFile.readlines():
			line=line.rstrip()
			m=re.match('>(.*)',line)
			if m:
				filterName=m.group(1)
				filterDict[filterName]=dict()
			elif line!='#':
				lineList=line.split('=')
				filterDict[filterName][lineList[0]]=lineList[1]
	for filterName in filterDict.keys():
		with open(rootedDir.reports+'/'+filterName+'_stat.tab','w') as statFile:
			statFile.write("\t".join(['kgID','before','after'])+"\n")
			for kgID in kgPathDict.keys():
				alnFileName=kgPathDict[kgID]+'/'+filterName+'/codon_aln.fa'
				alnBlockFileName=kgPathDict[kgID]+'/'+filterName+'/codon_aln_blocks.fa'
				if os.path.isfile(alnFileName) and os.path.isfile(alnBlockFileName):
					with open(alnFileName) as alnFile:
						line=alnFile.readline()
						line=alnFile.readline()
						beforeLen=str(len(line.rstrip()))
					with open(alnBlockFileName) as alnBlockFile:
						line=alnBlockFile.readline()
						line=alnBlockFile.readline()
						afterLen=str(len(line.rstrip()))
				else:
					afterLen='NA'
					beforeLen='NA'
				statFile.write("\t".join([kgID,beforeLen,afterLen])+"\n")
### Annotation
if 'annotate' in modeList:
	allSymbol=dict()
	iterNoName=1
	
	cnx = mysql.connector.connect(user='genome',host=args.host,port=args.port,database='hg19')
	for key in kgPathDict.keys():
		gene_name=Get_annotation(key)
	cnx.close()

	with open(rootedDir.reports+'/all_withCounts.txt','w') as logFile:
		for key in allSymbol:
			logFile.write(key+"\t"+str(allSymbol[key])+"\n")
	with open(rootedDir.reports+'/duplicate.txt','w') as logFile:
		for key in allSymbol:
			if allSymbol[key]>0:
				logFile.write(key+"\n")
	submit('cd '+rootedDir.path+' ; for kgID in $(ls results) ; do echo -e "$kgID\t$(cat results/$kgID/annotation/consName.txt)" ; done > reports/consName.tab')
	submit('cd '+rootedDir.reports+' ; for path in $(awk \'{ print $2 }\' kgPathDictFile.tab); do cat $path/annotation/consName.txt | sed -r "s/$/\t$(echo $path | sed \'s/\//\\\//g\')\n/g" ; done > consNameDict.tab')

	dirList=os.listdir(rootedDir.results)
	goDict=dict()
	goList=list()
	headGO=['goId','name','type','genes']
	keggDict=dict()
	keggList=list()
	headKegg=['mapId','name','genes']
	keggFileName=rootedDir.reports+'/kegg.tab'
	goFileName=rootedDir.reports+'/go.tab'

	for dirName in dirList:
		with open(rootedDir.results+'/'+dirName+'/annotation/consName.txt') as consNameFile:
			gene=consNameFile.readline().rstrip()
		File=open(rootedDir.results+'/'+dirName+'/annotation/go.tab','r')
		File.readline()
		for line in File.readlines():
			line=line.rstrip()
			lineList=line.split("\t")
			goId=lineList[1]
			name=lineList[2]
			gotype=lineList[3]
			if goId in goDict.keys():
				goDict[goId].append(gene)
			else:
				goDict[goId]=[gene]
				goList.append([goId,name,gotype])
		File.close()
		File=open(rootedDir.results+'/'+dirName+'/annotation/kegg.tab','r')
		File.readline()
		for line in File.readlines():
			line=line.rstrip()
			lineList=line.split("\t")
			mapId=lineList[1]
			name=lineList[2]
			if mapId in keggDict.keys():
				keggDict[mapId].append(gene)
			else:
				keggList.append([mapId,name])
				keggDict[mapId]=[gene]
		File.close()
	keggFile=open(keggFileName,'w')
	keggFile.write("\t".join(headKegg)+"\n")
	goFile=open(goFileName,'w')
	goFile.write("\t".join(headGO)+"\n")
	for lineList in keggList:
		mapId=lineList[0]
		keggFile.write("\t".join(lineList)+"\t"+','.join(keggDict[mapId])+"\n")
	for lineList in goList:
		goId=lineList[0]
		goFile.write("\t".join(lineList)+"\t"+','.join(goDict[goId])+"\n")
	goFile.close()
	keggFile.close()

## end of annotation

saveRoot(rootedDir)
sys.exit(0)


